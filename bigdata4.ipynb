{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2yHWkkHSDyf"
   },
   "source": [
    "# MRJOB\n",
    "\n",
    "## Co se naučíte?\n",
    "\n",
    "* seznámíte se s knihovnou MRJOB\n",
    "* vyzkoušíte ladící režim (nevyžaduje Hadoop)\n",
    "* spustíte MRJOB v dokerizovaném Hadoopu\n",
    "\n",
    "[MRJob](https://mrjob.readthedocs.io/en/latest/index.html#) je knihovna pro Python, která nám umožňuje snadno psát úlohy typu *Map* a *Reduce*. Její výhodou je poměrně slušná dokumentace a hlavně zde odpadá nutnost mít nainstalovaný *Hadoop*, protože je možné ji spouštět i lokálně. Z mého pohledu jako plus hodnotím i to, že je použit objektový přístup a uživatel si pouze podědí od bázové třídy a doplní příslušné metody.\n",
    "\n",
    "**Poznámka**\n",
    "\n",
    "Pro otestování uvedených příkladů stačí překopírovat buňky do pythonovského souboru a pak normálně spouštět z příkazové řádky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdmE_vJFT1nu"
   },
   "source": [
    "## Map Reduce - Hello World\n",
    "\n",
    "Téměř ve všech materiálech věnovaných Map a Reduce (*MR*), je ukázáno počítání počtu slov v nějakém dokumentu. Pojďme si na této úloze ilustrovat danou knihovnu *MRJob*.\n",
    "\n",
    "Na ukázce níže je zmiňovaná implementace počítání počtu slov. Nejprve je vytvořena třída, která dědí od *MRJob*, pak už stačí jen redefinovat metody *mapper* a *reducer*. Metoda *mapper* slouží k tomu, že přijímá textová data a provádí na nich transformaci tak, že získáme dvojici *klíč* a *hodnota*. V \"našem\" příkladu je vždy vytvořena dvojice slovo a číslo jedna. Metodou *reduce* právě provádíme operace nad hodnotami příslušejícími daným klíčům. Tedy *reducer* dostává ke každému klíči **generátor** hodnot. V níže uvedeném příkladu jsou pak sečteny všechny hodnoty, tedy všechny jedničky a dostáváme počet výskytů daného slova. V tomto minimalistickém příkladu bylo vynechána metoda *combiner*, která slouží ke slučování výsledků přicházejících z *mapperu*. \n",
    "\n",
    "Poslední částí je pak zavolání metody *run* pro spuštění."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdf0VLo5UjGm"
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "\n",
    "class WordCounter(MRJob):\n",
    "\tdef mapper(self, _, line):\n",
    "\t\tfor word in line.split():\n",
    "\t\t\tyield(word, 1)\n",
    "\tdef reducer(self, word, counts):\n",
    "\t\tyield(word, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tWordCounter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atcu6QD2YWI8"
   },
   "source": [
    "V dalším předpokládám, že program se jmenuje *wc.py* a spustíme jej v terminálu příkazem, kde *data.txt* je soubor, který chceme zpracovávat. Výstup bude produkován na standardní výstup *STDOUT*.\n",
    "\n",
    "```\n",
    "python3 wc.py -r inline data.txt\n",
    "\n",
    "```\n",
    "\n",
    "**Poznámka**\n",
    "\n",
    "Parametry příkazové řádky nám umožňují spouštět MRJob ve více módech. Pro nás nejzajímavější jsou *inline*, který je spouštěn v jednom procesu a je určen pro testování, zároveň je defaultní volbou, více viz [zde](https://mrjob.readthedocs.io/en/latest/runners-inline.html). Druhá volba je *local*, zde je již využito spuštění na více procesech, více viz [zde](https://mrjob.readthedocs.io/en/latest/runners-local.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krRoclODcDIM"
   },
   "source": [
    "## Map Reduce - Četnosti slov\n",
    "\n",
    "Nyní si ukážeme mírně komplikovanější úlohu. Program určí *n* slov s největší četností a pak nakreslí jejich historgram. Nejprve se podívejme na určování četností. Pro určení četností je nutné udělat více průchodů, tedy více operací map a reduce. Každý průchod map, combine, reduce se nazývá *step* (stupeň), viz [MRStep](https://mrjob.readthedocs.io/en/latest/step.html?highlight=mrstep) a výstupy jednotlivých stupňů jsou takto skládány. Napojení těchto stupňů se odehrává v metodě *steps*.\n",
    "\n",
    "Na ukázce níže jsou použity dva stupně, kde první stupeň používá všechny tři metody map, combine, reduce a druhý jen reduce. V mapperu prvního stupně (*mapper_map_words*) jsou jednotlivé řádky parsovány pomocí regulárních výrazů, viz [zde](https://docs.python.org/3/library/re.html), a každému slovu je přiřazena jednička. Jednotlivých instancí mapperů může obecně být více a každý bude zpracovávat část úlohy, proto se obvykle nad nimi píše metoda combine (*combiner*), která výsledky skládá a posílá dále. Vede to často ke zmenšení objemu přenášených dat. V naší ukázce v kombineru sčítáme počet výskytů nad daným slovem. První step je ukončen operací reduce, kde jsou opět sčítány výskyty slov, tedy výsledky z combine a následně je vytvořen generátor, který **nevrací klíč**, viz *None* na dané pozici a jako příslušná hodnota je použita dvojice slovo a četnost.\n",
    "\n",
    "**Poznámka**\n",
    "\n",
    "Lze to udělat i tak, že budeme vracet dvojici slovo a počet výskytů. Jako **samostatný úkol** upravte daný kód. Pozor, bude nutné upravit i reducer ve druhém stupni.\n",
    "\n",
    "Druhý stupeň je tvořen samostatným reducerem, který už jen výsledky setřídí a propustí deset nejčetnějších slov.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oq6601SbSACE"
   },
   "outputs": [],
   "source": [
    "# import knihoven a nastaveni\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "rex = re.compile(r\"\\b[\\w']{2,}\") # vytvoreni regularniho vyrazu, chci slova delsi nez dva znaky\n",
    "nth = 10 # kolik nejcastejsich slov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO0puIWk2Mur"
   },
   "outputs": [],
   "source": [
    "# vlastni implementace úlohy\n",
    "class Count(MRJob):\n",
    "    def mapper_map_words(self,_, line):\n",
    "        for word in rex.findall(line):\n",
    "                yield (word.lower(), 1)\n",
    "    def combiner_combine_words_count(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "    def reducer_max_word(self,_,count_word_pair):\n",
    "        for count, word in sorted(count_word_pair, reverse=True)[:nth]:\n",
    "            yield (word, count)\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_map_words,\n",
    "                   combiner=self.combiner_combine_words_count,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_max_word)\n",
    "        ]\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    Count.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqJgdmVz9D8M"
   },
   "source": [
    "Pro otestování jsme si stáhněte například román [RUR](https://www.gutenberg.org/ebooks/59112) a uložili jej jako *rur.txt*. Za předpokladu, že program se jmenuje *wordhistogram.py* jej spustíme následujícím kódem. Všimněte si přesměrování výsledků do souboru (*>*) *wordresults.txt*.\n",
    "```\n",
    "python3 wordhistogram.py -r local rur.txt > wordresults.txt\n",
    "```\n",
    "Dále je již jen vykreslení výsledků z daného souboru.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw5iPd48-85Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    data = pd.read_csv(\"wordresults.txt\", sep=\"\\t\", header=None, index_col=0)\n",
    "    print(data.head())\n",
    "    ax = data.plot.bar(legend='')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(\"Cetnosti slov\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dII11g69_M_Q"
   },
   "source": [
    "## Map Reduce - Průměrná teplota \n",
    "\n",
    "Nyní si ukážeme mírně komplikovanější úlohu, kde budeme číst ze vstupního *csv* souboru údaje o počasí, budeme počítat dení průměry a určíme počet dní v měsíci, kdy byla teplota vyšší než daná hodnota. Předpokládáme data v csv formátu, viz tento [soubor](https://1url.cz/@jf_meteo), který si ovšem musíte rozbalit.\n",
    "\n",
    "Výše zmíněná komplikace spočívá v tom, že nepočítáme průměry za celý den, ale jen pro nějaké pevně stanovené časy v 7, 14 a 21 hodin. \n",
    "Další komplikace spočívá v tom, že operací map může být spušteno více a není možné počítat průměr přímo v kombineru nad touto operací a pak ještě v reduceru, neboť by to počítalo průměr průměrů.\n",
    "\n",
    "V kódu níže je jedno z možných řešení. Opět jsou použity dva stupně. V prvním stupni se pracuje na úrovni dnů. Nejprve dojde, v operaci map (*mapper_map_day_temperature*) k naparsování info o počasí a filtrování hodnot na příslušné časy. Nakonec je vytvořen generátor, kde klíč je pouze datum (den) a hodnota je teplota ve stanovenou hodnotu. V následném combineru (*combiner_combine_temp*) je pro každý den vracena dvojice součet teplot a počet sečtených hodnot. Reducer prvního stupně (*reducer_filter_temperature*) opět přijímá tyto údaje a už počítá celkový průměr teploty v daném dni. Pokud je teplota vyšší než zadaná teplota (*temperature_filter*) přiřadí k danému dni hodnotu jedna, v opačném případě nula, čímž dochází k filtrování hodnot.\n",
    "\n",
    "Druhý stupeň pouze počítá počet dnů průměrnou teplotou vyšší než je zadaná teplota. Je to analogické určení počtu slov. Jediné co je třeba udělat je převádět klíče z formátu dnů na měsíce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfPQYl_HAVDb"
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from _datetime import datetime\n",
    "\n",
    "header_line = \"datetime\"\n",
    "datetime_index = 0\n",
    "outtemp_index = 3\n",
    "\n",
    "selected_hours = set([7, 14, 21])\n",
    "temperature_filter = 20.0\n",
    "\n",
    "\n",
    "class Count(MRJob):\n",
    "\n",
    "    def mapper_map_day_temperature(self, _, line):\n",
    "        items = line.split(',')\n",
    "        if items[datetime_index] != 'datetime':  # preskoc prvni radek\n",
    "            date = datetime.strptime(items[datetime_index], '%Y-%m-%d %H:%M:%S')\n",
    "            if date.hour in selected_hours and date.minute == 0:\n",
    "                yield (f\"{date.date()}\", float(items[outtemp_index]))  # date.date() float(items[2])\n",
    "\n",
    "    def combiner_combine_temp(self, day, temperatures):\n",
    "        suma = 0\n",
    "        counter = 0\n",
    "        for value in temperatures:\n",
    "            suma += value\n",
    "            counter += 1\n",
    "        yield (day, (suma, counter))\n",
    "\n",
    "    def reducer_filter_temperature(self, day, val):\n",
    "        c = 0\n",
    "        total = 0\n",
    "        for s, n in val:\n",
    "            c += n\n",
    "            total += s\n",
    "        # zda ma prumernou teplotu - za vsechna mereni\n",
    "        if total/c >= temperature_filter:\n",
    "            yield  (day, 1) #(day, f\"{total / c}-->{1}\")\n",
    "        else:\n",
    "            yield (day, 0) #(day, f\"{total / c}-->{0}\")\n",
    "\n",
    "    def mapper_map_day_to_month(self, day, val):\n",
    "        yield (day[:7], val)\n",
    "\n",
    "    def combiner_sum_days(self, month, val):\n",
    "        yield (month, sum(val))\n",
    "\n",
    "    def reducer_sum_days(self, month, val):\n",
    "        yield (month, sum(val))\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_map_day_temperature,\n",
    "                   combiner=self.combiner_combine_temp,\n",
    "                   reducer=self.reducer_filter_temperature),\n",
    "            MRStep(mapper=self.mapper_map_day_to_month,\n",
    "                   combiner=self.combiner_sum_days,\n",
    "                   reducer=self.reducer_sum_days)\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Count.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGh7ADosEORd"
   },
   "source": [
    "Za předpodkladu, že stažený soubor se jmenuje *meteo.csv* a daný skript *weather.py* spustíme z příkazové řádky následovně:\n",
    "\n",
    "```\n",
    " python3 weather.py -r local meteo.csv\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuované spuštění\n",
    "\n",
    "Pro distribuované prostředí je potřeba mít nakonfigurováno prostředí pro Hadoop streaming (tj. skript musí být spustitelný na všech uzlech) včetně dostupnosti `mrjob` a vstupní data musí být v HDFS.\n",
    "\n",
    "Vlastní spuštění je pak už snadné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python3 weather.py -r hadoop hdfs://user/root/meteo.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Úkoly**: Vyzkoušejte distribované zpracování a porovnejte jeho efektivitu s lokálním řešením (zohledněte to, že v případě využití docker verze je zpracování de facto stále lokální)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MRJOB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
