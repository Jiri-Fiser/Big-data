{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae8ffbd",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5571e8",
   "metadata": {},
   "source": [
    "## Co se naučíte?\n",
    "\n",
    "* instalaci dockerizovaného Sparku\n",
    "* seznámíte se základní datovou abstrakcí (RDD — *resilient distributed data set*)\n",
    "* prozkoumáte možnosti nízkoúrovňových operací nad RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87a4af",
   "metadata": {},
   "source": [
    "## Co je Spark\n",
    "\n",
    "I když se přístup, který zavedl Hadoop ukázal jako podnětný, přináší i jistá omezení:\n",
    "\n",
    "* data (a to i mezivýsledky) jsou ukládány do HDFS, který sice nabízí robustnost, ale i vyšší režii (ta se pro jevuje především při zpracování dat)\n",
    "* základní model MapReduce není příliš flexibilní a nelte jej aplikovat na některé třídy problémů\n",
    "* Java je jediný jazyk, který je v Hadoopu nativně podporován, ostatní jazyky musí využívat jen nepříliš elegantní proudové rozhraní\n",
    "* obtížné ladění programů (nelze je ladit bez minimálně tří běžících služeb)\n",
    "\n",
    "Z tohoto důvodu vzniklo několik řešení, které se při zachování výhod Hadoopu (distribuovanost dat i výpočtů) snaží nabídnout efektivnější a flexibilnější model.\n",
    "\n",
    "Spark patří mezi ty nejambicioznější, neboť poskytuje vysokoúrovňovou platformu s podporou několika programovacích jazyků, několika API a vlastní implementací výpočetního clusteru (tj. bez závislosti na Hadoopu).\n",
    "\n",
    "Základní charakteristikou Sparku je distribuované ukládání mezivýsledků do operační paměti. Tyto tzv. *resilient distributed dataset* jsou při zachování odolnosti proti výpadkům mnohem efektivnější než HDFS soubory. Na druhou stranu jej jejich velikost omezena na gigabyty (nejsou tak vhodné pro opravdu enormní datové sady)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b3b06",
   "metadata": {},
   "source": [
    "## Instalace\n",
    "\n",
    "Programy napsané pro Apache Spark lze vykonávat i bez instalace distribuovaného prostředí, neboť Apache Spark lze provozovat ve třech základních režimech:\n",
    "\n",
    "1. lokálně = pracovní uzly běží jako lokální procesy \n",
    "2. prostřednictvím samostatného Spark (v tomto případě je již nutné zajistit distribuci dat a Pythonského prostředí)\n",
    "3. prostřednictvím Hadoop správce zdrojů a plánovače Yarn (a s vyuužití HDFS pro sdílení dat i kódu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4a508",
   "metadata": {},
   "source": [
    "Lokální běh sice neumožňuje využít sílu distribuovaného zpracování, hodí se však pro účely ladění. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc0cbf",
   "metadata": {},
   "source": [
    "Nejjednodušším krokem je instalace klintská části. My zvolíme *pyspark* (další možností jsou klienti pro Scalu a R). \n",
    "Ten se instaluje jako běžný pythonský framework např. prostřednictvím `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1f0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/fiser/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (3.2.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/fiser/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from pyspark) (0.10.9.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0f3eb",
   "metadata": {},
   "source": [
    "Instalací se kromě příslušných modulů nainstaluje i interaktivní (REPL) prostředí `pyspark`. Spusťte ho a tím ověříte, že klient je dobře nainstalován.\n",
    "\n",
    "Po spuštění můžete také zkontrolovat jaký režim je právě využíván:\n",
    "\n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.9.7 (default, Oct 27 2021 01:23:21)\n",
    "Spark context Web UI available at http://192.168.122.1:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1652515703360).\n",
    "SparkSession available as 'spark'.\n",
    ">>> \n",
    "```\n",
    "\n",
    "V tomto případě je aktivní režim `locaĺ[*]` tj. běh v  několika souběžných lokálních procesech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f2769",
   "metadata": {},
   "source": [
    "Pokud chcete vyzkoušet distribuované prostředí (i když de facto stále omezené na jednoho hostitele) použijte docker\n",
    "obraz na adrese https://github.com/big-data-europe/docker-spark.\n",
    "\n",
    "> **Úkol**: Nainstalujte si tento obraz a prostudujte dokumentaci, jak ho používat.\n",
    "\n",
    "Pro spuštění můžete využít následující zjednodušenou `docker-compose` konfiguraci:\n",
    "\n",
    "```yaml\n",
    "ersion: '3'\n",
    "services:\n",
    "  spark-master:\n",
    "    image: bde2020/spark-master:3.2.0-hadoop3.2\n",
    "    container_name: spark-master\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "      - \"7077:7077\"\n",
    "    environment:\n",
    "      - INIT_DAEMON_STEP=setup_spark\n",
    "  spark-worker-1:\n",
    "    image: bde2020/spark-worker:3.2.0-hadoop3.2\n",
    "    container_name: spark-worker-1\n",
    "    depends_on:\n",
    "      - spark-master\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      - \"SPARK_MASTER=spark://spark-master:7077\"\n",
    "```\n",
    "\n",
    "Obecně vždy potřebujete službu master a alespoň jednu službu `worker`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0697947",
   "metadata": {},
   "source": [
    "Po spuštění se zkuste z `pyspark` připojit k serveru `master` (IP adresa se může měnit, použijte `docker network inspect docker-spark_default` pro její zjištění).\n",
    "\n",
    "```\n",
    "pyspark --master \"spark://172.20.0.2:7077\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3cdb3",
   "metadata": {},
   "source": [
    "Jak master tak worker zobrazují stav na na URL `http://localhost:8080` resp. `http://localhost:8081` (podívejte se na ně)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e6c1a",
   "metadata": {},
   "source": [
    "> **Úkol**: Přidejte další worker uzel v `docker-compose.yml`. Nezapomeňte změnit externí port, aby se nepřekrýval s prvním uzlem (např. na 8082). S touto konfigurací bude nadále pracovat.\n",
    "\n",
    "pro ty, kterým to připadá triviální: \n",
    "\n",
    "```\n",
    "dmVyc2lvbjogJzMnCnNlcnZpY2VzOgogIHNwYXJrLW1hc3RlcjoKICAgIGltYWdlOiBiZGUyMDIw\n",
    "L3NwYXJrLW1hc3RlcjozLjIuMC1oYWRvb3AzLjIKICAgIGNvbnRhaW5lcl9uYW1lOiBzcGFyay1t\n",
    "YXN0ZXIKICAgIHBvcnRzOgogICAgICAtICI4MDgwOjgwODAiCiAgICAgIC0gIjcwNzc6NzA3NyIK\n",
    "ICAgIGVudmlyb25tZW50OgogICAgICAtIElOSVRfREFFTU9OX1NURVA9c2V0dXBfc3BhcmsKICBz\n",
    "cGFyay13b3JrZXItMToKICAgIGltYWdlOiBiZGUyMDIwL3NwYXJrLXdvcmtlcjozLjIuMC1oYWRv\n",
    "b3AzLjIKICAgIGNvbnRhaW5lcl9uYW1lOiBzcGFyay13b3JrZXItMQogICAgZGVwZW5kc19vbjoK\n",
    "ICAgICAgLSBzcGFyay1tYXN0ZXIKICAgIHBvcnRzOgogICAgICAtICI4MDgxOjgwODEiCiAgICBl\n",
    "bnZpcm9ubWVudDoKICAgICAgLSAiU1BBUktfTUFTVEVSPXNwYXJrOi8vc3BhcmstbWFzdGVyOjcw\n",
    "NzciCiAgc3Bhcmstd29ya2VyLTI6CiAgICBpbWFnZTogYmRlMjAyMC9zcGFyay13b3JrZXI6My4y\n",
    "LjAtaGFkb29wMy4yCiAgICBjb250YWluZXJfbmFtZTogc3Bhcmstd29ya2VyLTIKICAgIGRlcGVu\n",
    "ZHNfb246CiAgICAgIC0gc3BhcmstbWFzdGVyCiAgICBwb3J0czoKICAgICAgLSAiODA4Mjo4MDgx\n",
    "IgogICAgZW52aXJvbm1lbnQ6CiAgICAgIC0gIlNQQVJLX01BU1RFUj1zcGFyazovL3NwYXJrLW1h\n",
    "c3Rlcjo3MDc3Igo=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928aec3",
   "metadata": {},
   "source": [
    "Nyní už můžeme vytvářet první programy pro Spark. Začneme klíčovou abstrakcí frameworku Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e5adc",
   "metadata": {},
   "source": [
    "## Resilient distributed dataset\n",
    "\n",
    "Na nejnižší úrovni programování se ve Sparku pracuje s *resilient distributed datasets*:\n",
    "\n",
    "1) jsou to datové sady = posloupnosti n-tic (tj. de facto tabulky) včetně posloupnosti řetězců (řádků) a posloupnosti bloků bytů (podporovány jsou elementární typy i komplexní objekty podporovaných jazyků, lze )\n",
    "2) jsou distribuované na jednotlivé uzly (po částech, tzv. *partitioning*, k dispozici je ale i replikace)\n",
    "3) jsou odolné proti závadám na jednotlivých uzlech (fault tolerant) \n",
    "\n",
    "mezi další vlastnosti patří:\n",
    "* neměnnost (aplikací mapovací operace vznikne nové RDD)\n",
    "* úložištěm je primárně operační paměť (v případě většího objemu dat, či požadavku presistentního kešování však mouho být uložena i v distrubuovaném FS)\n",
    "* mapovací operace nad RDD jsou lenivé (provedou se až tehdy, kdy je očekáván výsledek)\n",
    "\n",
    "RDD mohou vzniknout dvěma způsoby:\n",
    "\n",
    "* paralelizací existujícího kontejneru (v Pythonu typicky seznamu nebo numpy pole)\n",
    "* čtením ze souboru (typicky ze diatribuovaného souborového systému HDFS).\n",
    "\n",
    "Pro otestování programového přístupu vyzkoušíme elementární úlohu -- paralelizaci numpy pole (čísla 0 až 999) a následná shromáždění do seznamu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1808f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "\n",
    "conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
    "with  SparkContext(conf=conf) as sc:\n",
    "    data = np.arange(0, 10_000, dtype=np.int16) #list(range(1_000))\n",
    "    print(type(data))\n",
    "    rdd = sc.parallelize(data, 8) # objekt representuje rozddistribuované RDD\n",
    "    print(type(rdd))\n",
    "    cdata = rdd.collect()\n",
    "    print(cdata[:100]) # vypíšeme jen prvních sto položek\n",
    "    print(type(cdata)) # cdate je seznam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51019c37",
   "metadata": {},
   "source": [
    "Jádrem programu je vytvoření tzv. spark-kontextu. Ten je vytvořen na základě konfigurace, která v tomto případě uvádí jen jméno úlohy (*job*) a režim vykonávání. Jméno úlohy se zobrazuje v případě použití distribuovaného systému a režim je v tomto případě jen lokální (spuštění ve Spark clusteru si necháme na zajímavější program).\n",
    "\n",
    "RDD vznikne paralelizací numpy pole (je možné využít i seznam či jinou sekvenci = indexovatelnou kolekci) metodou nad objektem kontextu. Další metody se volají již přímo nad RDD objektem. Zde je použita  metoda `collect`, která shromáždí data RDD ze všech výpočetních uzlů.\n",
    "\n",
    "Context je nutno ukončit a uvolnit, což v tomto případě zajišťuje konstrukce `with`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8923c0",
   "metadata": {},
   "source": [
    "Reálnější (i když datově opět neadekvátně malý) ukazuje jednoduché zpracování CSV souboru.\n",
    "\n",
    "Nejdříve je přečten textový soubor, tak že výsledkem je RDD tvořené posloupností záznamů (partitiong nicméně neprobíhá po jednotlivých záznamech ale v blocích jednotek stovek MiB, tj. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66af7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.txt\n",
    "\n",
    "2022-05-12,10,3\n",
    "2022-05-13,8,2\n",
    "2022-05-14,11,1\n",
    "2022-05-15,14,-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8dd7d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from operator import add\n",
    "\n",
    "conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
    "with  SparkContext(conf=conf) as sc:\n",
    "    rdd = sc.textFile(\"data.txt\")\n",
    "    t_rdd = rdd.filter(lambda line: line.strip()).map(lambda line: int(line.split(\",\")[2]))\n",
    "    result = t_rdd.reduce(add) # redukujeme součtem\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387208d",
   "metadata": {},
   "source": [
    "Nejdříve se provede vytvoření RDD ze souboru. To se nicméně prozatím neprovede , jen se naplánuje. V dalším kroku se provedou dvě **transformace**. První odfiltruje prázdné řádky, druhá je rozloží a vrátí třetí sloupec jako číslo. Všechny transformace jsou lenivé a tak ani na tomto řádku se nic neprovede.\n",
    "\n",
    "Posledním krokem je redukce operací `add` (= obdoba operátoru `+` v podobě funkce). To je **akce**, která způsobí vykonání všech transformací a vstupních metod na nichž závisí (kromě různých typů redukcí do akcí patří i metoda `collect`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80857f19",
   "metadata": {},
   "source": [
    "> **Úkol**: Upravte kód tak, aby počítal průměr. (existuje několik řešení). Nabídku možných transformací najdete na například na https://spark.apache.org/docs/latest/rdd-programming-guide.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa34684",
   "metadata": {},
   "source": [
    "Spuštění ve dockerizovaném Spark clusteru, který se víc blíží konečnému nasazení (i když stále není možné mluvit o nějakém skutečně distribuovaném zpracování) není přímočaré.\n",
    "\n",
    "Za prvé je nutné program uložit jako skript (do stejného adresáře jako soubor `data.txt`). Všimněte si, že se v něm změnila cesta k datovému souboru (to není chyba)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d092abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from operator import add\n",
    "\n",
    "conf = SparkConf().setAppName(\"test\")\n",
    "with  SparkContext(conf=conf) as sc:\n",
    "    rdd = sc.textFile(\"/app/app.py\")\n",
    "    t_rdd = rdd.filter(lambda line: line.strip()).map(lambda line: int(line.split(\",\")[2]))\n",
    "    result = t_rdd.reduce(add) # redukujeme součtem\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfcc64",
   "metadata": {},
   "source": [
    "V dalším kroku je nutné v témže adresáři vytvořit Dockerfile odvozený z bde2020/spark-python-template. (dokumentace viz popis šablony na githubu).\n",
    "\n",
    "```Dockerfile\n",
    "FROM bde2020/spark-python-template:3.2.0-hadoop3.2\n",
    "\n",
    "ENV SPARK_APPLICATION_PYTHON_LOCATION /app/app.py\n",
    "```\n",
    "\n",
    "Dále je nutné v tomto adresáři vytvořit soubor `requirements.txt` se jmény nestandardních modulů, jež je nutno nainstalovat přes `pip` (musí být vytvořen i když zůstane prázdný).\n",
    "\n",
    "Ten poté přeložit:\n",
    "\n",
    "```bash\n",
    "docker build --rm -t bde/spark-app .\n",
    "```\n",
    "\n",
    "A spustit:\n",
    "\n",
    "```bash\n",
    "docker run --rm --name my-spark-app -e ENABLE_INIT_DAEMON=false --link spark-master:spark-master \n",
    "           --net docker-spark_default bde/spark-app:latest\n",
    "```\n",
    "\n",
    "\n",
    "Výsledek vypadá takto. Je zřejmé, že se spouští celá mašinerie, v jejímž výstupu se vlastní výsledek ztrácí (pokuste se ho najít:))\n",
    "```\n",
    "Submit application /app/app.py to Spark master spark://spark-master:7077\n",
    "Passing arguments \n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "22/05/15 20:56:21 INFO SparkContext: Running Spark version 3.2.0\n",
    "22/05/15 20:56:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "22/05/15 20:56:22 INFO ResourceUtils: ==============================================================\n",
    "22/05/15 20:56:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
    "22/05/15 20:56:22 INFO ResourceUtils: ==============================================================\n",
    "22/05/15 20:56:22 INFO SparkContext: Submitted application: test\n",
    "22/05/15 20:56:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
    "22/05/15 20:56:22 INFO ResourceProfile: Limiting resource is cpu\n",
    "22/05/15 20:56:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
    "22/05/15 20:56:22 INFO SecurityManager: Changing view acls to: root\n",
    "22/05/15 20:56:22 INFO SecurityManager: Changing modify acls to: root\n",
    "22/05/15 20:56:22 INFO SecurityManager: Changing view acls groups to: \n",
    "22/05/15 20:56:22 INFO SecurityManager: Changing modify acls groups to: \n",
    "22/05/15 20:56:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
    "22/05/15 20:56:22 INFO Utils: Successfully started service 'sparkDriver' on port 42269.\n",
    "22/05/15 20:56:22 INFO SparkEnv: Registering MapOutputTracker\n",
    "22/05/15 20:56:22 INFO SparkEnv: Registering BlockManagerMaster\n",
    "22/05/15 20:56:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "22/05/15 20:56:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "22/05/15 20:56:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
    "22/05/15 20:56:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c972acf7-e388-49be-9a39-0c89636a6943\n",
    "22/05/15 20:56:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
    "22/05/15 20:56:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "22/05/15 20:56:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
    "22/05/15 20:56:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://b30927379e1d:4040\n",
    "22/05/15 20:56:23 INFO Executor: Starting executor ID driver on host b30927379e1d\n",
    "22/05/15 20:56:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44891.\n",
    "22/05/15 20:56:23 INFO NettyBlockTransferService: Server created on b30927379e1d:44891\n",
    "22/05/15 20:56:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
    "22/05/15 20:56:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b30927379e1d, 44891, None)\n",
    "22/05/15 20:56:23 INFO BlockManagerMasterEndpoint: Registering block manager b30927379e1d:44891 with 366.3 MiB RAM, BlockManagerId(driver, b30927379e1d, 44891, None)\n",
    "22/05/15 20:56:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b30927379e1d, 44891, None)\n",
    "22/05/15 20:56:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b30927379e1d, 44891, None)\n",
    "22/05/15 20:56:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.1 KiB, free 366.0 MiB)\n",
    "22/05/15 20:56:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
    "22/05/15 20:56:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b30927379e1d:44891 (size: 32.0 KiB, free: 366.3 MiB)\n",
    "22/05/15 20:56:24 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
    "22/05/15 20:56:24 INFO FileInputFormat: Total input files to process : 1\n",
    "22/05/15 20:56:24 INFO SparkContext: Starting job: reduce at /app/app.py:9\n",
    "22/05/15 20:56:24 INFO DAGScheduler: Got job 0 (reduce at /app/app.py:9) with 1 output partitions\n",
    "22/05/15 20:56:24 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /app/app.py:9)\n",
    "22/05/15 20:56:24 INFO DAGScheduler: Parents of final stage: List()\n",
    "22/05/15 20:56:24 INFO DAGScheduler: Missing parents: List()\n",
    "22/05/15 20:56:24 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at reduce at /app/app.py:9), which has no missing parents\n",
    "22/05/15 20:56:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KiB, free 365.9 MiB)\n",
    "22/05/15 20:56:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 365.9 MiB)\n",
    "22/05/15 20:56:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b30927379e1d:44891 (size: 5.2 KiB, free: 366.3 MiB)\n",
    "22/05/15 20:56:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427\n",
    "22/05/15 20:56:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at reduce at /app/app.py:9) (first 15 tasks are for partitions Vector(0))\n",
    "22/05/15 20:56:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
    "22/05/15 20:56:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b30927379e1d, executor driver, partition 0, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
    "22/05/15 20:56:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
    "22/05/15 20:56:25 INFO HadoopRDD: Input split: file:/app/data.txt:0+65\n",
    "22/05/15 20:56:25 INFO PythonRunner: Times: total = 423, boot = 410, init = 13, finish = 0\n",
    "22/05/15 20:56:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1440 bytes result sent to driver\n",
    "22/05/15 20:56:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1127 ms on b30927379e1d (executor driver) (1/1)\n",
    "22/05/15 20:56:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
    "22/05/15 20:56:25 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44781\n",
    "22/05/15 20:56:25 INFO DAGScheduler: ResultStage 0 (reduce at /app/app.py:9) finished in 1.272 s\n",
    "22/05/15 20:56:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
    "22/05/15 20:56:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
    "22/05/15 20:56:25 INFO DAGScheduler: Job 0 finished: reduce at /app/app.py:9, took 1.332424 s\n",
    "3\n",
    "22/05/15 20:56:25 INFO SparkUI: Stopped Spark web UI at http://b30927379e1d:4040\n",
    "22/05/15 20:56:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "22/05/15 20:56:25 INFO MemoryStore: MemoryStore cleared\n",
    "22/05/15 20:56:25 INFO BlockManager: BlockManager stopped\n",
    "22/05/15 20:56:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
    "22/05/15 20:56:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "22/05/15 20:56:25 INFO SparkContext: Successfully stopped SparkContext\n",
    "22/05/15 20:56:26 INFO ShutdownHookManager: Shutdown hook called\n",
    "22/05/15 20:56:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca1f275f-da48-48ca-a4c2-4ee29d4c547a/pyspark-342607bc-5980-4461-995f-5e3974e557ee\n",
    "22/05/15 20:56:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca1f275f-da48-48ca-a4c2-4ee29d4c547a\n",
    "22/05/15 20:56:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9f763c7-fd12-4a43-8e80-56442b7fffa7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993eea4",
   "metadata": {},
   "source": [
    "Tato šablona zajišťuje tři klíčové podmínky pro spuštění aplikace:\n",
    "    \n",
    "1) aplikace musí být spuštěna pomocí příkazu spark-submit, který zajišťuje vykonání tzv. driveru (řídící program na uzlu master) a dílčích úloh v uzlech `worker`.\n",
    "\n",
    "2) identické softwarové prostředí (stejná verze Pythonu, stejná nabídka knihoven apod.)\n",
    "\n",
    "navíc je nutné zajistit, že všechny pracovní uzly uvidí uzel na stejné URL adrese. V reálném nasazení je to většinou sdílený HDFS, lze však například využít i HTTP(S). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436aca5",
   "metadata": {},
   "source": [
    "> **Úkol:** Vytvořte program, který pro náš textový soubor (resp. rozsáhlejší soubor se stejnou strukturou) vrátí průměrnou hodnotu posledního sloupce a spusťtě jej na (dockerizovaném) Spark clusteru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53146d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
